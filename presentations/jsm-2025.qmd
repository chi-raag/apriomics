---
title: Automated Prior Elicitation for Bayesian Metabolomics Analysis
subtitle: JSM 2025 | Flexible Prior Elicitation for Bayesian Analysis 
author: Chiraag Gohel
date: August 6, 2025
institute: The Rahnavard Lab, The George Washington University
logo: images/gw_primary_2c_0.png
bibliography: references.bib
csl: https://www.zotero.org/styles/american-statistical-association
suppress-bibliography: true
fig-dpi: 150
format: 
    revealjs: 
        theme: gw.scss
        html-math-method: mathjax
        slide-number: true
mermaid-format: svg
---

## What is metabolomics?

![From Human Metabolome Technologies](https://humanmetabolome.com/ap/wp-content/uploads/2020/11/metabolomics_01.png){width="80%"}

::: {.notes}
- Comprehensively profile small-molecule metabolites in cells, tissues, or biofluids to capture the biochemical “phenotype” in real time.
- Link upstream omics to downstream phenotype: bridge the gap between genomic/proteomic signals and observable physiology or disease states.
- Detect pathway-level perturbations (e.g., energy, lipid, or amino-acid metabolism) that underlie exposures, interventions, or pathology.
- Discover and validate biomarkers for diagnosis, prognosis, or treatment response
:::


## Effect size drives biological insight

![](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0291798.g001&type=large)

## Traditional testing lacks power

:::: {.columns}

::: {.column width="70%"}

![](images/univariate_simulation.png)

:::

::: {.column width="30%" .fragment}

![](https://upload.wikimedia.org/wikipedia/en/e/ec/Human_Metabolome_Database_logo.png){width="60%"}

![](https://pubchem.ncbi.nlm.nih.gov/pcfe/logo/PubChem_logo.png){width="60%"}

:::{.fragment}
- High dimensionality ($p >> n$)
- Can lean on assumptions of sparsity
- Prior knowledge from previous studies, literature, and curated databases
:::

:::

::::

## Prior work

![Capstick, A., Krishnan, R. G., and Barnaghi, P. (2025), [“AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling,”](https://doi.org/10.48550/arXiv.2411.17284.) arXiv.](https://arxiv.org/html/2411.17284v5/x1.png)

![Zhang, E., Goto, R., Sagan, N., Mutter, J., Phillips, N., Alizadeh, A., Lee, K., Blanchet, J., Pilanci, M., and Tibshirani, R. (2025), [“LLM-Lasso: A Robust Framework for Domain-Informed Feature Selection and Regularization,”](https://doi.org/10.48550/arXiv.2502.10648) arXiv.](https://arxiv.org/html/2502.10648v2/x1.png){width="80%" fig-align="center"}

## Prior elicitation framework overview

```{mermaid}
graph TD;
    A[LLM] --> B[Metabolite + Study Context]
    B --> C[LLM Prior Elicitation]
    C --> D[Bayesian Model]
    D --> E[Posterior Inference]
    E --> F[Biological Insights]
```

## LLM prior elicitation process

**Step 1**: LLM analyzes metabolite + study context
$$\text{LLM}(\text{metabolite}, \text{condition}) \rightarrow \{d_j, m_j, c_j, r_j\}$$

where $d_j \in \{\text{increase, decrease, unchanged}\}$ is predicted direction, $m_j \in \{\text{small, moderate, large}\}$ is predicted magnitude, $c_j \in \left(0, 1\right)$ is confidence level, and $r_j$ is a string representing the rationale.

**Step 2**: Map qualitative predictions to numerical priors

\begin{align*}
\mu_j^{\text{LLM}} &= f_{\mu}\left(m_j, d_j\right) \\
\sigma_j^{\text{LLM}} &= f_{\sigma}\left(c_j\right)
\end{align*}

**Step 3**: Use as informative priors in Bayesian model
$\beta_j \sim \mathcal{N}(\mu_j^{\text{LLM}}, \sigma_j^{\text{LLM}})$

## Priors

**LLM Priors**: $\beta_j \sim \mathcal{N}(\mu_j^{\text{LLM}}, \sigma_j^{\text{LLM}})$

where $\mu_j^{\text{LLM}}$ and $\sigma_j^{\text{LLM}}$ are derived from LLM predictions:

\begin{align}
\mu_j^{\text{LLM}} &= m_j \cdot \text{sign}(d_j) \\
\sigma_j^{\text{LLM}} &= f(c_j)
\end{align}

:::: {.columns}

::: {.column width="45%"}
**Conservative Mapping**

$m_j \in \{0.08, 0.15, 0.25\}$ for $\{\text{small, moderate, large}\}$

$f(c_j) \in \{0.5, 0.7, 0.9\}$ for $\{\text{high, med, low}\}$ confidence
:::

::: {.column width="45%"}
**Moderate Mapping**
$m_j \in \{0.12, 0.22, 0.35\}$ for $\{\text{small, moderate, large}\}$

$f(c_j) \in \{0.3, 0.5, 0.7\}$ for $\{\text{high, med, low}\}$ confidence
:::

::::

**Oracle Prior (Upper Bound)**: $\beta_j \sim \mathcal{N}(\beta_j^{\text{true}}, 0.25)$

**Weakly Informative Prior**: $\beta_j \sim \mathcal{N}(0, 2)$

## LLM-Informed Hierarchical Prior

Group metabolites by LLM predictions and use intelligent pooling:

\begin{align*}
\text{Group means}_g &\sim \mathcal{N}\bigl(\mu_g^{\mathrm{LLM}}, 3.0\bigr) \\
\beta_j &\sim \mathcal{N}\bigl(\text{Group means}_{g[j]}, 2.0\bigr)
\end{align*}

where group $g$ is mapped to $\mu_g^{\mathrm{LLM}}$ as follows:

\begin{align*}
\mu_g^{\mathrm{LLM}} =
\begin{cases}
-0.1, & \text{if }g = \text{decrease},\\
0.0,  & \text{if }g = \text{unchanged},\\
+0.1, & \text{if }g = \text{increase}.
\end{cases}
\end{align*}


## Modeling

All Bayesian models use the same log-link GLM structure with different prior specifications:

\begin{align}
y_{ij} &\sim \mathcal{N}(\mu_{ij}, \sigma_j^2) \\
\log(\mu_{ij}) &= \alpha_j + \beta_j \cdot x_i \\
\alpha_j &\sim \mathcal{N}(\log(\bar{y}_j), 1.0) \\
\sigma_j &\sim \text{HalfNormal}(0.5)
\end{align}

where $y_{ij}$ is abundance for sample $i$ and metabolite $j$, $x_i \in \{0,1\}$ is group indicator, and $\beta_j$ represents the natural log fold change (lnFC) for metabolite $j$.

## Simulation Study

### Empirical Monte-Carlo Subsampling

```{mermaid} 
%%| fig-height: 3
---
config:
    theme: 'base'
    themeVariables:
        fontFamily: Avenir Next LT Pro
        primaryColor: '#F8E08E'
        secondaryColor: '#0190DB'
---
graph TD;
    A[Full Dataset] --> B[Subsample Data]
    A --> C[Calculate true lnFC]
    B --> D[Fit Models]
    E[Weakly Informative Priors] --> D
    F[LLM Priors] --> D
    G[HMDB + LLM Priors] --> D
    D --> H[Compare model estimators]
    C --> H

classDef prior fill:white;
class E,F,G prior;
```

**Ground Truth**: Empirical natural log fold change (lnFC) from full MTBLS1 dataset^[Salek, R. M., Maguire, M. L., Bentley, E., Rubtsov, D. V., Hough, T., Cheeseman, M., Nunez, D., Sweatman, B. C., Haselden, J. N., Cox, R. D., Connor, S. C., and Griffin, J. L. (2007), [“A metabolomic comparison of urinary changes in type 2 diabetes in mouse, rat, and human,”](https://doi.org/10.1152/physiolgenomics.00194.2006.) Physiological Genomics, American Physiological Society.] (n=132)

$$\beta_j^{\text{true}} = \log\left(\frac{\bar{y}_j^{\text{case}}}{\bar{y}_j^{\text{control}}}\right)$$



## LLM-informed priors improve recovery

```{r}
#| fig-width: 15
library(tidyverse)
library(ggplot2)

data <- read_csv("../output/benchmark_prior_recovery_results.csv")

data_filtered <- data %>%
    filter(Method != "LLM-Informed Hierarchical") %>%
    mutate(
        llm = case_when(
            str_detect(Method, "Flash 2.0") ~ "Flash 2.0",
            str_detect(Method, "Pro") ~ "Pro 2.5",
            str_detect(Method, "GPT-4o") ~ "GPT-4o",
            str_detect(Method, "O3 Mini") ~ "O3 Mini",
            str_detect(Method, "Uninformative") ~ "Uninformative",
            str_detect(Method, "Oracle") ~ "Oracle",
            TRUE ~ "O3"
        ),
        llm = factor(llm, levels = c("Uninformative", "Flash 2.0", "GPT-4o", "O3", "O3 Mini", "Pro 2.5", "Oracle")),
        context = case_when(
            str_detect(Method, "With Context") ~ "Context",
            str_detect(Method, "No Context") ~ "No Context",
            TRUE ~ NA
        ),
        short_ctx = case_when(
            context == "Context" ~ "C",
            context == "No Context" ~ "N"
        )
    )

sample_labeller <- labeller(
    sample_size = c(
        `5`  = "n = 5",
        `10` = "n = 10",
        `15` = "n = 15",
        `20` = "n = 20"
    )
)

label_data <- data_filtered %>%
    filter(!is.na(context)) %>%
    group_by(sample_size, Method, context, short_ctx) %>%
    summarize(max_rmse = max(rmse, na.rm = TRUE), .groups = "drop") %>%
    group_by(sample_size) %>%
    mutate(y_pos = max(max_rmse) * 1) %>%
    ungroup()

data_filtered %>%
    mutate(Method = factor(Method, levels = c(
        "Uninformative Bayesian",
        "Flash 2.0 (No Context, Conservative)", "Flash 2.0 (With Context, Conservative)",
        "GPT-4o (No Context, Conservative)", "GPT-4o (With Context, Conservative)",
        "O3 (No Context, Conservative)",
        "O3 Mini (No Context, Conservative)", "O3 Mini (With Context, Conservative)",
        "Pro 2.5 (No Context, Conservative)", "Pro 2.5 (With Context, Conservative)",
        "Oracle Bayesian"
    ))) %>%
    ggplot(aes(x = Method, y = rmse, fill = llm)) +
    geom_boxplot(outliers = FALSE) +
    geom_text(
        inherit.aes = FALSE,
        data = label_data,
        aes(x = Method, y = y_pos, label = short_ctx, color = context),
        size = 3,
        show.legend = TRUE
    ) +
    scale_color_manual(
        name = "Context",
        values = c(
            "Context" = "black",
            "No Context" = "black"
        ),
        breaks = c("Context", "No Context"),
        labels = c("With Context", "No Context"),
        guide = guide_legend(
            override.aes = list(label = c("C", "N"), size = 4)
        )
    ) +
    facet_grid(~sample_size, scales = "free_x", labeller = sample_labeller) +
    theme_bw(base_size = 20, base_family = "Avenir Next LT Pro") +
    theme(
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank(),
        axis.line.x = element_blank()
    ) +
    labs(
        y     = "Root Mean Squared Error (RMSE)",
        fill  = "LLM Method",
        color = "Context"
    )
```

$$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{j=1}^{n} (\hat{\beta}_j - \beta_j^{\text{true}})^2}$$

where $\hat{\beta}_j$ is the estimated effect size and $\beta_j^{\text{true}}$ is the ground truth lnFC for metabolite $m_j$.

## LLM Informed estimators are finite-sample efficient

```{r}
data_bias_var <- read_csv("../output/benchmark_bias_variance_results.csv")

# compute arrow endpoints on the same filtered subset
arrow_coords <- data_bias_var |>
    filter(method %in% c("o3_no_context_conservative", "uninformative_bayesian", "oracle_bayesian")) |>
    summarise(
        min_bias = min(overall_bias, na.rm = TRUE),
        min_var  = min(overall_variance, na.rm = TRUE),
        max_bias = max(overall_bias, na.rm = TRUE),
        max_var  = max(overall_variance, na.rm = TRUE)
    ) |>
    mutate(
        good_x = min_bias + 0.2 * (max_bias - min_bias),
        good_y = min_var + 0.2 * (max_var - min_var),
        bad_x  = min_bias + 0.8 * (max_bias - min_bias),
        bad_y  = min_var + 0.8 * (max_var - min_var)
    )

data_bias_var |>
    filter(method %in% c("o3_no_context_conservative", "uninformative_bayesian", "oracle_bayesian")) |>
    mutate(
        method = case_when(
            method == "o3_no_context_conservative" ~ "O3 No Context",
            method == "uninformative_bayesian" ~ "Uninformative Bayesian",
            method == "oracle_bayesian" ~ "Oracle Bayesian"
        )
    ) |>
    ggplot(aes(x = overall_bias, y = overall_variance, color = method)) +
    geom_point(size = 6) +
    annotate(
        "text",
        x = -Inf, y = -Inf,
        label = "Low bias & variance",
        hjust = -0.5, vjust = -.5,
        size = 3, color = "darkgreen",
        family = "Avenir Next LT Pro"
    ) +
    annotate(
        "text",
        x = Inf, y = Inf,
        label = "High bias & variance",
        hjust = 1.5, vjust = 1.5,
        size = 3, color = "red",
        family = "Avenir Next LT Pro"
    ) +
    # arrows pointing into the Good/Bad corners
    geom_segment(
        data = arrow_coords,
        aes(x = good_x, y = good_y, xend = min_bias, yend = min_var),
        arrow = arrow(length = unit(0.2, "inches")),
        color = "darkgreen"
    ) +
    geom_segment(
        data = arrow_coords,
        aes(x = bad_x, y = bad_y, xend = max_bias, yend = max_var),
        arrow = arrow(length = unit(0.2, "inches")),
        color = "red"
    ) +
    facet_wrap(~sample_size, labeller = labeller(sample_labeller)) +
    scale_color_brewer(palette = "Dark2", name = "Method") +
    scale_shape_manual(values = c(16, 17, 15), name = "Method") +
    labs(
        x = "Overall Bias",
        y = "Overall Variance"
    ) +
    theme_bw(base_family = "Avenir Next LT Pro")
```

## Summary

- **LLM Prior Elicitation Works**: Automated biological knowledge extraction via LLMs produces informative priors for Bayesian metabolomics analysis.
- **Mapping Strategy Matters**: Magnitude-driven effect sizes and confidence-calibrated uncertainties are crucial for translating qualitative LLM insights into effective numerical priors.
- **Practical Impact**: Method particularly valuable for small sample studies (n=5-20) where traditional statistical approaches struggle with high-dimensional metabolomics data.
- **Future Directions**: Integration of other databases, alongside more sophisticated mapping approaches and historical data.