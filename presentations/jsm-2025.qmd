---
title: Automated Prior Elicitation for Bayesian Metabolomics Analysis
subtitle: JSM 2025 | Flexible Prior Elicitation for Bayesian Analysis 
author: Chiraag Gohel
date: August 6, 2025
institute: The Rahnavard Lab, The George Washington University
logo: images/gw_primary_2c_0.png
bibliography: references.bib
csl: https://www.zotero.org/styles/american-statistical-association
suppress-bibliography: true
fig-dpi: 150
format: 
    revealjs: 
        theme: gw.scss
        html-math-method: mathjax
        slide-number: true
        title-slide-attributes:
            data-background-image: https://dtais.engineering.gwu.edu/sites/g/files/zaxdzs4921/files/header_block/DTAIS%20logo_1c.png, https://rahnavard.org/media/logo_hu6032439a0cdf518af9f282061b335860_10275_0x70_resize_lanczos_3.png
            data-background-size: 210px, 75px
            data-background-position: 87% 99%, 93% 99%
        width: 1600
        height: 900
        pdf-separate-fragments: true
mermaid-format: svg
---

## What is metabolomics?

![From Human Metabolome Technologies](https://humanmetabolome.com/ap/wp-content/uploads/2020/11/metabolomics_01.png){width="80%"}

::: {.notes}
- Comprehensively profile small-molecule metabolites in cells, tissues, or biofluids to capture the biochemical “phenotype” in real time.
- Link upstream omics to downstream phenotype: bridge the gap between genomic/proteomic signals and observable physiology or disease states.
- Detect pathway-level perturbations (e.g., energy, lipid, or amino-acid metabolism) that underlie exposures, interventions, or pathology.
- Discover and validate biomarkers for diagnosis, prognosis, or treatment response
:::


## Effect size drives biological insight

:::: {.columns}

::: {.column width="70%"}
![](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0291798.g001&type=large)
:::

::: {.column width="30%" .incremental}
- Enables pathway-level modeling: Effect sizes aggregate naturally (e.g., via gene-set enrichment or mixed models) to quantify pathway shifts, supporting causal inference.
- Facilitates reproducibility & meta-analysis: Confidence intervals around effect sizes allow results to be combined across labs, instruments, and platforms—key for harmonizing heterogeneous metabolomics studies.
:::

::::

## Traditional testing lacks power

:::: {.columns}

::: {.column width="70%"}

![Henglin, M. et al. (2022), “Quantitative Comparison of Statistical Methods for Analyzing Human Metabolomics Data,” Metabolites, 12, 519. 
](images/univariate_simulation.png)

:::

::: {.column width="30%" .fragment}

![](https://upload.wikimedia.org/wikipedia/en/e/ec/Human_Metabolome_Database_logo.png){width="60%"}

![](https://pubchem.ncbi.nlm.nih.gov/pcfe/logo/PubChem_logo.png){width="60%"}

:::{.fragment}

**Can we use LLMs as "experts" to inform priors in Bayesian metabolomics models?**

:::

:::

::::

## Prior work

:::{.fragment}
![Zhang, E. et al. (2025), [“LLM-Lasso: A Robust Framework for Domain-Informed Feature Selection and Regularization,”](https://doi.org/10.48550/arXiv.2502.10648) arXiv.](https://arxiv.org/html/2502.10648v2/x1.png){width="80%" fig-align="center"}
:::

:::{.fragment}
![Capstick, A. et al. (2025), [“AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling,”](https://doi.org/10.48550/arXiv.2411.17284.) arXiv.](https://arxiv.org/html/2411.17284v5/x1.png){width="80%"}
:::

## Prior elicitation framework overview

![](./images/overview.png){width="100%"}

::: {.incremental}

- Do LLMs require extra knowledge from databases or literature?
- How do different prompt formulations affect effect size estimation?
- Can this process be automated for high-throughput metabolomics studies?

:::


## LLM prior elicitation process

::: {.fragment}
**Step 1**: LLM analyzes metabolite + study context
$$\text{LLM}(\text{metabolite}, \text{condition}) \rightarrow \{d_j, m_j, c_j, r_j\}$$

where $d_j \in \{\text{increase, decrease, unchanged}\}$ is predicted direction, $m_j \in \{\text{small, moderate, large}\}$ is predicted magnitude, $c_j \in \left(0, 1\right)$ is confidence level, and $r_j$ is a string representing the rationale.
:::

::: {.fragment}
**Step 2**: Map qualitative predictions to numerical priors

\begin{align*}
\mu_j^{\text{LLM}} &= f_{\mu}\left(m_j, d_j\right) \\
\sigma_j^{\text{LLM}} &= f_{\sigma}\left(c_j\right)
\end{align*}
:::

::: {.fragment}
**Step 3**: Use as informative priors in Bayesian model
$\beta_j \sim \mathcal{N}(\mu_j^{\text{LLM}}, \sigma_j^{\text{LLM}})$
:::

## Priors

**LLM Priors**: $\beta_j \sim \mathcal{N}(\mu_j^{\text{LLM}}, \sigma_j^{\text{LLM}})$

where $\mu_j^{\text{LLM}}$ and $\sigma_j^{\text{LLM}}$ are derived from LLM predictions:

\begin{align}
\mu_j^{\text{LLM}} &= m_j \cdot \text{sign}(d_j) \\
\sigma_j^{\text{LLM}} &= f(c_j)
\end{align}

:::: {.columns}

::: {.column width="45%"}
**Conservative Mapping**

$m_j \in \{0.08, 0.15, 0.25\}$ for $\{\text{small, moderate, large}\}$

$f(c_j) \in \{0.5, 0.7, 0.9\}$ for $\{\text{high, med, low}\}$ confidence
:::

::: {.column width="45%"}
**Moderate Mapping**
$m_j \in \{0.12, 0.22, 0.35\}$ for $\{\text{small, moderate, large}\}$

$f(c_j) \in \{0.3, 0.5, 0.7\}$ for $\{\text{high, med, low}\}$ confidence
:::

::::

**Oracle Prior (Upper Bound)**: $\beta_j \sim \mathcal{N}(\beta_j^{\text{true}}, 0.25)$

**Weakly Informative Prior**: $\beta_j \sim \mathcal{N}(0, 2)$

## LLM-Informed Hierarchical Prior {visibility="hidden"}

Group metabolites by LLM predictions and use intelligent pooling:

\begin{align*}
\text{Group means}_g &\sim \mathcal{N}\bigl(\mu_g^{\mathrm{LLM}}, 3.0\bigr) \\
\beta_j &\sim \mathcal{N}\bigl(\text{Group means}_{g[j]}, 2.0\bigr)
\end{align*}

where group $g$ is mapped to $\mu_g^{\mathrm{LLM}}$ as follows:

\begin{align*}
\mu_g^{\mathrm{LLM}} =
\begin{cases}
-0.1, & \text{if }g = \text{decrease},\\
0.0,  & \text{if }g = \text{unchanged},\\
+0.1, & \text{if }g = \text{increase}.
\end{cases}
\end{align*}


## Modeling

All Bayesian models use the same log-link GLM structure with different prior specifications:

\begin{align}
y_{ij} &\sim \mathcal{N}(\mu_{ij}, \sigma_j^2) \\
\log(\mu_{ij}) &= \alpha_j + \beta_j \cdot x_i \\
\alpha_j &\sim \mathcal{N}(\log(\bar{y}_j + \epsilon), 1.0) \\
\sigma_j &\sim \text{HalfNormal}(0.5)
\end{align}

where $y_{ij}$ is abundance for sample $i$ and metabolite $j$, $x_i \in \{0,1\}$ is group indicator, $\beta_j$ represents the natural log fold change (lnFC) for metabolite $j$, and $\epsilon$ is a small constant to avoid log(0).

## Simulation Study

### Empirical Monte-Carlo Subsampling

```{mermaid} 
%%| fig-height: 5
%%| fig-width: 14
---
config:
    theme: 'base'
    themeVariables:
        fontFamily: Mona Sans
        primaryColor: '#F8E08E'
        secondaryColor: '#0190DB'
---
graph TD;
    A[Full Dataset] --> B[Subsample Data]
    A --> C[Calculate true lnFC]
    B --> D[Fit Models]
    E[Weakly Informative Priors] --> D
    F[LLM Priors] --> D
    G[HMDB + LLM Priors] --> D
    D --> H[Compare estimators]
    C --> H

classDef prior fill:white;
class E,F,G prior;
```

**Ground Truth**: Empirical natural log fold change (lnFC) from full MTBLS1 dataset^[Salek, R. M. et al. (2007), [“A metabolomic comparison of urinary changes in type 2 diabetes in mouse, rat, and human,”](https://doi.org/10.1152/physiolgenomics.00194.2006.) Physiological Genomics, American Physiological Society.]: $\beta_j^{\text{true}} = \log\left(\frac{\bar{y}_j^{\text{case}}}{\bar{y}_j^{\text{control}}}\right)$ (n=132)

# Results

## Examples of LLM Output

:::: {layout="[30, -30, 10, -20]"}

**Acetoacetate**

![](https://www.sigmaaldrich.com/deepweb/assets/sigmaaldrich/product/structures/344/649/4f016901-2115-4196-8191-7aeee6a5f0f3/640/4f016901-2115-4196-8191-7aeee6a5f0f3.png)

::::

:::: {.columns}

::: {.column width="50%"}
WITH HMDB Context:

- Prediction: increase
- Magnitude: moderate
- Confidence: 0.85
- Reasoning: "In type 2 diabetes, insulin resistance diminishes the hormone's antilipolytic effect, leading to increased free fatty acid release and subsequent hepatic ketogenesis. This process boosts the production of ketone bodies including acetoacetate. Although the patients have good dietary control and are not in full ketoacidosis, the metabolic imbalance typical of T2DM still promotes a moderate increase in urinary acetoacetate relative to healthy controls."
:::

::: {.column width="50%"}
WITHOUT HMDB Context:

- Prediction: increase
- Magnitude: moderate
- Confidence: 0.8
- Reasoning: "Acetoacetate is a ketone body produced during fatty acid oxidation. In type 2 diabetes, even with good dietary control, insulin resistance promotes increased lipolysis which mildly enhances ketogenesis. Although these patients are not in overt ketoacidosis, the metabolic shift can lead to a moderate increase in urinary acetoacetate."
:::

::::

## LLM-informed priors improve recovery

```{r}
#| fig-width: 16
#| fig-height: 5

library(tidyverse)
library(ggnewscale)
library(ggplot2)
library(gghighlight)

data <- read_csv("../output/benchmark_prior_recovery_results.csv")

data_filtered <- data %>%
    filter(Method != "LLM-Informed Hierarchical") %>%
    mutate(
        llm = case_when(
            str_detect(Method, "Flash 2.0") ~ "Flash 2.0",
            str_detect(Method, "Pro") ~ "Pro 2.5",
            str_detect(Method, "GPT-4o") ~ "GPT-4o",
            str_detect(Method, "O3 Mini") ~ "O3 Mini",
            str_detect(Method, "Uninformative") ~ "Uninformative",
            str_detect(Method, "Oracle") ~ "Oracle",
            TRUE ~ "O3"
        ),
        llm = factor(llm, levels = c("Uninformative", "Flash 2.0", "GPT-4o", "O3", "O3 Mini", "Pro 2.5", "Oracle")),
        context = case_when(
            str_detect(Method, "With Context") ~ "Context",
            str_detect(Method, "No Context") ~ "No Context",
            TRUE ~ NA
        ),
        short_ctx = case_when(
            context == "Context" ~ "C",
            context == "No Context" ~ "N"
        )
    )

sample_labeller <- labeller(
    sample_size = c(
        `5`  = "n = 5",
        `10` = "n = 10",
        `15` = "n = 15",
        `20` = "n = 20"
    )
)

label_data <- data_filtered %>%
    filter(!is.na(context)) %>%
    group_by(sample_size, Method, context, short_ctx) %>%
    summarize(max_rmse = max(rmse, na.rm = TRUE), .groups = "drop") %>%
    group_by(sample_size) %>%
    mutate(y_pos = max(max_rmse) * 1) %>%
    ungroup()

p <- data_filtered %>%
    mutate(Method = factor(Method, levels = c(
        "Uninformative Bayesian",
        "Flash 2.0 (No Context, Conservative)", "Flash 2.0 (With Context, Conservative)",
        "GPT-4o (No Context, Conservative)", "GPT-4o (With Context, Conservative)",
        "O3 (No Context, Conservative)",
        "O3 Mini (No Context, Conservative)", "O3 Mini (With Context, Conservative)",
        "Pro 2.5 (No Context, Conservative)", "Pro 2.5 (With Context, Conservative)",
        "Oracle Bayesian"
    ))) %>%
    ggplot(aes(x = Method, y = rmse, fill = llm)) +
    geom_boxplot(outliers = FALSE)

p +
    gghighlight(llm == "Uninformative", unhighlighted_params = list(fill = NULL, alpha = 0.3), keep_scales = TRUE, calculate_per_facet = TRUE) +
    geom_text(
        inherit.aes = FALSE,
        data = label_data,
        aes(x = Method, y = y_pos, label = short_ctx, color = context),
        size = 3,
        show.legend = TRUE
    ) +
    scale_color_manual(
        name = "Context",
        values = c(
            "Context" = "black",
            "No Context" = "black"
        ),
        breaks = c("Context", "No Context"),
        labels = c("With Context", "No Context"),
        guide = guide_legend(
            override.aes = list(label = c("C", "N"), size = 4)
        )
    ) +
    geom_point(aes(x = Method, y = rmse, fill = llm), shape = 22, alpha = .5, show.legend = FALSE) +
    facet_grid(~sample_size, scales = "free_x", labeller = sample_labeller) +
    theme_bw(base_size = 20, base_family = "Mona Sans") +
    theme(
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank(),
        axis.line.x = element_blank(),
        grid.panel = element_blank()
    ) +
    labs(
        y     = "Root Mean Squared Error (RMSE)",
        fill  = "LLM Method",
        color = "Context"
    )

```

$$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{j=1}^{n} (\hat{\beta}_j - \beta_j^{\text{true}})^2}$$

where $\hat{\beta}_j$ is the estimated effect size and $\beta_j^{\text{true}}$ is the ground truth lnFC for metabolite $m_j$.

## LLM-informed priors improve recovery

```{r}
#| fig-width: 16
#| fig-height: 5
p +
    gghighlight(llm == "Oracle", unhighlighted_params = list(fill = NULL, alpha = 0.3), keep_scales = TRUE, calculate_per_facet = TRUE) +
    geom_text(
        inherit.aes = FALSE,
        data = label_data,
        aes(x = Method, y = y_pos, label = short_ctx, color = context),
        size = 3,
        show.legend = TRUE
    ) +
    scale_color_manual(
        name = "Context",
        values = c(
            "Context" = "black",
            "No Context" = "black"
        ),
        breaks = c("Context", "No Context"),
        labels = c("With Context", "No Context"),
        guide = guide_legend(
            override.aes = list(label = c("C", "N"), size = 4)
        )
    ) +
    geom_point(aes(x = Method, y = rmse, fill = llm), shape = 22, alpha = .5, show.legend = FALSE) +
    facet_grid(~sample_size, scales = "free_x", labeller = sample_labeller) +
    theme_bw(base_size = 20, base_family = "Mona Sans") +
    theme(
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank(),
        axis.line.x = element_blank(),
        grid.panel = element_blank()
    ) +
    labs(
        y     = "Root Mean Squared Error (RMSE)",
        fill  = "LLM Method",
        color = "Context"
    )
```

$$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{j=1}^{n} (\hat{\beta}_j - \beta_j^{\text{true}})^2}$$

where $\hat{\beta}_j$ is the estimated effect size and $\beta_j^{\text{true}}$ is the ground truth lnFC for metabolite $m_j$.

## LLM-informed priors improve recovery

```{r}
#| fig-width: 16
#| fig-height: 5
p +
    geom_text(
        inherit.aes = FALSE,
        data = label_data,
        aes(x = Method, y = y_pos, label = short_ctx, color = context),
        size = 3,
        show.legend = TRUE
    ) +
    scale_color_manual(
        name = "Context",
        values = c(
            "Context" = "black",
            "No Context" = "black"
        ),
        breaks = c("Context", "No Context"),
        labels = c("With Context", "No Context"),
        guide = guide_legend(
            override.aes = list(label = c("C", "N"), size = 4)
        )
    ) +
    geom_point(aes(x = Method, y = rmse, fill = llm), shape = 22, alpha = .5, show.legend = FALSE) +
    facet_grid(~sample_size, scales = "free_x", labeller = sample_labeller) +
    theme_bw(base_size = 20, base_family = "Mona Sans") +
    theme(
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank(),
        axis.line.x = element_blank(),
        grid.panel = element_blank()
    ) +
    labs(
        y     = "Root Mean Squared Error (RMSE)",
        fill  = "LLM Method",
        color = "Context"
    )
```

$$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{j=1}^{n} (\hat{\beta}_j - \beta_j^{\text{true}})^2}$$

where $\hat{\beta}_j$ is the estimated effect size and $\beta_j^{\text{true}}$ is the ground truth lnFC for metabolite $m_j$.

## LLM Informed estimators are finite-sample efficient

```{r}
data_bias_var <- read_csv("../output/benchmark_bias_variance_results.csv")

# compute arrow endpoints on the same filtered subset
arrow_coords <- data_bias_var |>
    filter(method %in% c("o3_no_context_conservative", "uninformative_bayesian", "oracle_bayesian")) |>
    summarise(
        min_bias = min(overall_bias, na.rm = TRUE),
        min_var  = min(overall_variance, na.rm = TRUE),
        max_bias = max(overall_bias, na.rm = TRUE),
        max_var  = max(overall_variance, na.rm = TRUE)
    ) |>
    mutate(
        good_x = min_bias + 0.2 * (max_bias - min_bias),
        good_y = min_var + 0.2 * (max_var - min_var),
        bad_x  = min_bias + 0.8 * (max_bias - min_bias),
        bad_y  = min_var + 0.8 * (max_var - min_var)
    )

data_bias_var |>
    filter(method %in% c("o3_no_context_conservative", "uninformative_bayesian", "oracle_bayesian")) |>
    mutate(
        method = case_when(
            method == "o3_no_context_conservative" ~ "O3 No Context",
            method == "uninformative_bayesian" ~ "Uninformative Bayesian",
            method == "oracle_bayesian" ~ "Oracle Bayesian"
        )
    ) |>
    ggplot(aes(x = overall_bias, y = overall_variance, color = method)) +
    geom_point(size = 6) +
    annotate(
        "text",
        x = -Inf, y = -Inf,
        label = "Low bias & variance",
        hjust = -0.5, vjust = -.5,
        size = 3, color = "darkgreen",
        family = "Avenir Next LT Pro"
    ) +
    annotate(
        "text",
        x = Inf, y = Inf,
        label = "High bias & variance",
        hjust = 1.5, vjust = 1.5,
        size = 3, color = "red",
        family = "Avenir Next LT Pro"
    ) +
    # arrows pointing into the Good/Bad corners
    geom_segment(
        data = arrow_coords,
        aes(x = good_x, y = good_y, xend = min_bias, yend = min_var),
        arrow = arrow(length = unit(0.2, "inches")),
        color = "darkgreen"
    ) +
    geom_segment(
        data = arrow_coords,
        aes(x = bad_x, y = bad_y, xend = max_bias, yend = max_var),
        arrow = arrow(length = unit(0.2, "inches")),
        color = "red"
    ) +
    facet_wrap(~sample_size, labeller = labeller(sample_labeller)) +
    scale_color_brewer(palette = "Dark2", name = "Method") +
    scale_shape_manual(values = c(16, 17, 15), name = "Method") +
    labs(
        x = "Overall Bias",
        y = "Overall Variance"
    ) +
    theme_bw(base_family = "Mona Sans")
```

## Summary

- **LLM Prior Elicitation Works**
- **Mapping Strategy Matters**
- **Added Context May Not Matter**
- **Performance is Model Agnostic**
- **Larger Gains at Small Sample Sizes**

## {background-image="images/ack.png" background-size="contain"}